{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset-eda1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbi/KAPO8V/84WjwMT5b8t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanmayyb/screw-detector/blob/main/dataset_eda1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "yVdVu-lRorHJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from scipy import ndimage\n",
        "from shapely.geometry import Point\n",
        "from shapely.geometry.polygon import Polygon\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijoXvwV81bcX",
        "outputId": "75267006-b9f4-4e39-c677-7cbafc118df6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd 'My Drive/screw_dataset/unpacked/'"
      ],
      "metadata": {
        "id": "k1AjylEQ3juf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#os.listdir()"
      ],
      "metadata": {
        "id": "w1oWCC7S3nX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('README_v1.0.txt') as f: print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OojShw3q3-U4",
        "outputId": "ace540e9-2c73-40a6-9de1-f3f89dca3f46"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************\n",
            "* MVTec Screws V1.0                       *\n",
            "*                                         *\n",
            "* Author: MVTec Software GmbH, July 2020. *\n",
            "*         https://www.mvtec.com           *\n",
            "*******************************************\n",
            "\n",
            "All files are as in the MVTec Screws example dataset for oriented object detection, released with\n",
            "HALCON version 19.05. The state of the dataset and images is as of release version 20.05.\n",
            "\n",
            "***********\n",
            "* License *\n",
            "***********\n",
            "\n",
            "The dataset, i.e. the images and the annotations, are licensed under the creative commons\n",
            "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. See\n",
            "https://creativecommons.org/licenses/by-nc-sa/4.0/ for more information.\n",
            "For using the data in a way that falls under the commercial use clause of the license,\n",
            "please contact us.\n",
            "\n",
            "***************\n",
            "* Attribution *\n",
            "***************\n",
            "\n",
            "If you use this dataset in scientific work, please cite our paper:\n",
            "\n",
            "Markus Ulrich, Patrick Follmann, Jan-Hendrik Neudeck: \n",
            "A comparison of shape-based matching with deep-learning-based object detection;\n",
            "in: Technisches Messen, 2019, DOI 10.1515/teme-2019-0076.\n",
            "\n",
            "************\n",
            "* Content: *\n",
            "************\n",
            "\n",
            "MVTec Screws contains 384 images of 13 different types of screws and nuts on a wooden background.\n",
            "The objects are labeled by oriented bounding boxes and their respective category. Overall, there\n",
            "are 4426 of such annotations.\n",
            "The exemplary splits are those that have been used in the above mentioned publication. Initially,\n",
            "they have been selected randomly, such that approximately 70% of the instances of each category are\n",
            "within the training split, and 15% each in the validation and test splits.\n",
            "\n",
            "* folder images contains the screw images.\n",
            "* mvtec_screws.json contains the annotations for all images in COCO format.\n",
            "* mvtec_screws_train/val/test.json contain examplary splits as mentioned above, in COCO format.\n",
            "* mvtec_screws.hdict contains the DLDataset unsplitted.\n",
            "* mvtec_screws_split.hdict contains the DLDataset with splits.\n",
            "\n",
            "******************************\n",
            "* Usage of DLDataset-format: *\n",
            "******************************\n",
            "\n",
            "The .hdict files can be used within HALCON by reading them, e.g. via\n",
            "\n",
            "read_dict (<path_to_mvtec_screws.hdict>, [], [], DLDataset)\n",
            "\n",
            "The image path has to be set to the location of the images folder <path_to_images_folder> by\n",
            "\n",
            "set_dict_tuple(DLDataset, 'image_dir', <path_to_images_folder>)\n",
            "\n",
            "To store this information within the dataset, the dataset should be written by\n",
            "write_dict (DLDataset, <path_to_mvtec_screws.hdict>, [], [])\n",
            "\n",
            "In HALCON object detection we use subpixel-precise annotations with a pixel-centered coordinate-system, i.e.\n",
            "the center of the top-left corner of the image is at (0.0, 0.0), while the top-left corner of the image is\n",
            "located at (-.5, -.5). Note that when used within HALCON the dataset does not need to be converted, as this\n",
            "format is also used within the deep learning based object detection of HALCON.\n",
            "\n",
            "***************\n",
            "* COCO Format *\n",
            "***************\n",
            "\n",
            "MVTec screws is a dataset for oriented box detection. We use a format that is very similar to that of the\n",
            "COCO dataset (cocodataset.org). However, we need 5 parameters per box annotation to store the orientation.\n",
            "We use the following labels.\n",
            "\n",
            "Each box contains 5 parameters (row, col, width, height, phi), where\n",
            "\n",
            "* 'row' is the subpixel-precise center row (vertical axis of the coordinate system) of the box.\n",
            "* 'col' is the subpixel-precise center column (horizontal axis of the coordinate system) of the box.\n",
            "* 'width' is the subpixel-precise width of the box. I.e. the length of the box parallel to the orientation \n",
            "  of the box.\n",
            "* 'height' is the subpixel-precise width of the box. I.e. the length of the box perpendicular to the\n",
            "  orientation of the box.\n",
            "* 'phi' is the orientation of the box in radian, given in a mathematically positive sense and with respect\n",
            "  to the horizontal (column) image axis. E.g. for phi = 0.0 the box is oriented towards the right side of \n",
            "  the image, for phi = pi/2 towards the top, for phi = pi towards the left, and for phi=-pi/2 towards the \n",
            "  bottom. Phi is always in the range (-pi, pi].\n",
            "\n",
            "Note that width and height are defined in contrast to the DLDataset format in HALCON, where we use \n",
            "semi-axis lengths.\n",
            "\n",
            "Coordinate system: In comparison to the pixel-centered coordinate-system of HALCON mentioned above,\n",
            "                   for COCO it is common to set the origin to the top-left-corner of the top-left\n",
            "\t\t\t\t   pixel, hence in comparison to the DLDataset-format, (row,col) are shifted by (.5, .5).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the json file with the annotation metadata\n",
        "with open('mvtec_screws.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "print(data.keys())\n",
        "print(data['images'][0])\n",
        "print(data['annotations'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vdhAmzX4trT",
        "outputId": "752df798-215c-4eaf-a956-0e3a5cd98be9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['categories', 'images', 'annotations', 'licenses', 'info'])\n",
            "{'file_name': 'screws_001.png', 'height': 1440, 'width': 1920, 'id': 1, 'license': 1}\n",
            "{'area': 3440.97, 'bbox': [184.5, 876.313, 55, 62.5631, 0], 'category_id': 7, 'id': 1001, 'image_id': 1, 'is_crowd': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data['images'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyGkTD-H5Rnq",
        "outputId": "c74a179a-0a77-4be1-b917-345cf1195601"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the images, and make some helpful dict to map the data\n",
        "imgdict  = {l['id']:l for l in data['images']}\n",
        "\n",
        "#make dict that contains images and metadata by id\n",
        "for i in imgdict.values():\n",
        "  i['image'] = io.imread(os.path.join('images', i['file_name']))[:,:,:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "vGN_NAz8PXQ1",
        "outputId": "7be031ff-caef-473e-8327-6e7ffac3f856"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-646a1bd9aff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#make dict that contains images and metadata by id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 100"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgdict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdGSO_7pS9Ej",
        "outputId": "8e8e86a9-830e-4b1a-8eb8-bb7480800890"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "io.imread(os.path.join('images', i['file_name']))[:,:,:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "4teXmT-oSLvF",
        "outputId": "7fa509a7-c6dd-40e7-fa91-c8b10ee000d4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-9980081342b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 100"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the images, and make some helpful dict to map the data\n",
        "imgdir = 'images'\n",
        "\n",
        "#remap images to dict by id\n",
        "imgdict = {l['id']:l for l in data['images']}\n",
        "#read in all images, can take some time\n",
        "for i in imgdict.values():\n",
        "  i['image'] = io.imread(os.path.join(imgdir, i['file_name']))[:, :,: 3]  # drop alpha channel, if it's there\n",
        "\n",
        "# remap annotations to dict by image_id\n",
        "from collections import defaultdict\n",
        "annodict = defaultdict(list)\n",
        "for annotation in data['annotations']:\n",
        "  annodict[annotation['image_id']].append(annotation)\n",
        "\n",
        "# setup list of categories\n",
        "categories = data['categories']\n",
        "ncategories = len(categories)\n",
        "cat_ids = [i['id'] for i in categories]\n",
        "category_names = {7:'nut', 3:'wood screw', 2:'lag wood screw', 8:'bolt',\n",
        "                  6:'black oxide screw', 5:'shiny screw', 4:'short wood screw',\n",
        "                  1:'long lag screw', 9:'large nut', 11:'nut', 10:'nut',\n",
        "                  12:'machine screw', 13:'short machine screw' }"
      ],
      "metadata": {
        "id": "2GHpvd-D5LYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}